{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz, itertools, unicodedata, json, calendar\n",
    "from datetime import datetime, timezone, timedelta, date\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "#from functools import reduce\n",
    "today = date.today()\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler um arquivo de configuração e aplicar como filtro\n",
    "\n",
    "config_file = '/mnt/sadatalakevcbr/config/Engemix/dualmining_taglist.csv'\n",
    "# lê arquivo de configuração com a lista de Tags\n",
    "config_tag_list = [ row[0] for row in spark.read.format(\"csv\").option(\"header\",True).load(config_file).collect()]\n",
    "\n",
    "# Lê dados e aplica filtro\n",
    "df_cleansed_tags = spark.read.format(\"delta\").load(source_path)\\\n",
    "                        .filter(col(\"tag\").isin(config_tag_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso de uso: \n",
    "# - Devo atualizar dados mas não recebo sinalização quando um arquivo é eliminado\n",
    "# - tenho uma data que posso usar (DT_REFERENCIA) e tenho a garantia que dados muito antigos a partir dessa data não mudam\n",
    "# - a carga para a camada raw é desacoplada, aqui falamos só do processamento do que foi carregado na ingestão\n",
    "# - Garantimos que em cada arquivo entregue pela ingestão é completo, \n",
    "#   os dados de uma data remessa presente no arquivo mais novo é o que conta, os demais devem ser descartados.\n",
    "\n",
    "# Solução: \n",
    "# - Carregar o conteúdo de n dias atras a partir dessa data, eliminar tudo antes de excluir.\n",
    "# - A parte de trazer os dados dos ultimos x dias é feita numa etapa anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtem o dado da camada de ingestão\n",
    "\n",
    "last_exec_date = date.today() # Só um exemplo, Usar para isso um sistema de controle de mudança de data\n",
    "key_fields = [\"NF\", \"SERIE_NF\"]\n",
    "insert_order_fields = \"TS_UTC_ADF_INGESTION\" # (yyyy-MM-dd HH:mm:ss)\n",
    "\n",
    "df = spark.read.schema(my_schema) \\\n",
    "      .csv(get_path(\"raw\", my_business_area, my_dataset_name, source_name), header=True) \\\n",
    "      .withColumn('SourceFileName', input_file_name())\\\n",
    "      .withColumn('ANO_REFERENCIA', substring(\"DT_REFERENCIA\", 1,4)) \\\n",
    "      .filter(col(\"DateIngest\") >= last_exec_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para cada data de referencia elimina duplicidades pela pelo nome do arquivo,\n",
    "# assim ficamos apenas com o dado do ultimo arquivo.\n",
    "# O nome do arquivo pode ser considerado como ordem cronológica.\n",
    "# dbfs:/mnt/sadatalakevcbr/raw/Engemix/Command/DadosCPS/YearIngest=2021/MonthIngest=09/DateIngest=2021-09-24/DadosCPS_UTC2021-09-24%20040023__2021-08-10_2021-09-24.csv|2021-09-24 04:00:24 \n",
    "\n",
    "#df.groupBy(['SOURCEFILENAME', \"TS_UTC_ADF_INGESTION\"]).count().orderBy(['SOURCEFILENAME', \"TS_UTC_ADF_INGESTION\"]).show(1000, False)\n",
    "\n",
    "w = Window.partitionBy('DT_REFERENCIA')\n",
    "df = df.withColumn('maxSourceFileName', max('SourceFileName').over(w)) \\\n",
    "       .where(col('maxSourceFileName') == col('SourceFileName')) \\\n",
    "       .drop('maxSourceFileName')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicidades pelos campos chave levando em consideração algum outro campo\n",
    "#window = Window.partitionBy(key_fields).orderBy([desc(order_field) for order_field in insert_order_fields])\n",
    "window = Window.partitionBy(key_fields).orderBy(desc(col('DATA_ULT_ATUALIZACAO')))\n",
    "df = df.withColumn(\"rank_temp\", row_number().over(window)).filter(col(\"rank_temp\") == 1).drop(\"rank_temp\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checagem:::: Busca Duplicidades, não deve mostrar nenhum registro\n",
    "\n",
    "window = Window.partitionBy(key_fields).orderBy([desc(order_field) for order_field in insert_order_fields])\n",
    "df.withColumn(\"num_temp\", lit(1))\\\n",
    "  .withColumn(\"quant_linhas\", sum(col(\"num_temp\")).over(window))\\\n",
    "  .drop(\"num_temp\")\\\n",
    "  .filter(col(\"quant_linhas\") > 1)\\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criacao da lista de novas datas a serem inseridas\n",
    "lista_data_remessa_excluir = [x.DT_REFERENCIA for x in df.select(col(\"DATA_REMESSA\").cast(\"string\")).distinct().orderBy(\"DATA_REMESSA\").collect()]\n",
    "lista_data_remessa_excluir = ','.join([f\"'{x}'\" for x in lista_data_remessa_excluir]) # Ex: datas = '\"2020-01-01\", \"2020-01-02\"'\n",
    "print(lista_data_remessa_excluir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grava os dados no camada Cleansed\n",
    "# antes elimina dados no destino que estejam na lista de datas que vamos inserir. \n",
    "# Com isso resolvemos o problema de registros excluídos na origem sendo mantidos no Data Lake\n",
    "# Vamos exluir e carregar novamente\n",
    "\n",
    "# obtem o path do destino\n",
    "path_dados_cps_cleansed = ''\n",
    "print(path_dados_cps_cleansed)\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, path_dados_cps_cleansed):\n",
    "  print (\"Atualizando delta table, passo 1/2: Eliminar dados antigos\")\n",
    "  delta_table = DeltaTable.forPath(spark, path_dados_cps_cleansed)\n",
    "  delta_table.delete(\"DT_REFERENCIA in ({0})\".format(lista_data_remessa_excluir))\n",
    "  print (\"Atualizando delta table, passo 2/2: Gravar dados novos\")\n",
    "  df.write.format(\"delta\").mode(\"append\").partitionBy('ANO_REFERENCIA').save(path_dados_cps_cleansed)\n",
    "else:\n",
    "  print (\"Criando delta table\")\n",
    "  df.write.format(\"delta\").partitionBy('ANO_REFERENCIA').save(path_dados_cps_cleansed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
