{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como trabalhar com stream\n",
    "\n",
    "Ver [essa explicação](https://docs.databricks.com/spark/latest/structured-streaming/production.html) para mais detalhes.\n",
    "\n",
    "Em resumo é recomendável utilizar um local para armazenar checkpoints assim você pode reiniciar a query de onde parou em caso de falhas.\n",
    "Esse local de checkpoint preserva toda informação essencial que identifica unicamente uma query. _Cada query deve ter um local de checkpoint exclusivo_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame.writeStream\n",
    "  .format(\"parquet\")\n",
    "  .option(\"path\", \"dbfs://outputPath/\")\n",
    "  .option(\"checkpointLocation\", \"dbfs://checkpointPath\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de stream\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preciso criar um exemplo melhor para isso.\n",
    "# esse caso ele avalia os arquivos que chegaram na entrada e processa automaticamente a cada execução.\n",
    "# o stream não precisa ficar ligado o tempo inteiro.\n",
    "\n",
    "INPUT_PATH = 'path/to/data'\n",
    "OUTPUT_PATH = 'path/to/data/output'\n",
    "CHECKPOINT_LOCATION_PATH = 'path/to/checkpoint'\n",
    "\n",
    "schema_tags = \"\"\"\n",
    "tag string,\n",
    "time string,\n",
    "value string,\n",
    "Year string,\n",
    "Month string,\n",
    "Date date\n",
    "\"\"\"\n",
    "\n",
    "dfStream = spark.readStream.format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.format\", \"csv\") \\\n",
    "  .option('sep', '|') \\\n",
    "  .schema(schema_tags) \\\n",
    "  .load(INPUT_PATH)\n",
    "\n",
    "def foreachBatchWriteCleansed(df, id):\n",
    "  # Validacao  se existem novos registros a serem processados\n",
    "  if df.first() is None:\n",
    "    return None\n",
    "\n",
    "# Criacao da lista de novas datas a serem inseridas (reduzir o numero de arquivos lidos pelo merge)\n",
    "  # https://kb.databricks.com/delta/delta-merge-into.html\n",
    "  datas = [x.Date for x in df_final.select(col(\"Date\").cast(\"string\")).distinct().collect()]\n",
    "  \n",
    "  datas = ','.join([f'\"{x}\"' for x in datas]) # Ex: datas = '\"2020-01-01\", \"2020-01-02\"'\n",
    "\n",
    "def create_tags_table_if_not_exist():\n",
    "  sql(f\"\"\"CREATE TABLE IF NOT EXISTS out_table\n",
    "  (\n",
    "  id string,\n",
    "  time timestamp,\n",
    "  value string,\n",
    "  Year integer,\n",
    "  Month string,\n",
    "  Date date\n",
    "  )\n",
    "  USING DELTA \n",
    "  PARTITIONED BY (Year, Month, Date)\n",
    "  LOCATION '{OUTPUT_PATH}'\"\"\"\n",
    "     )\n",
    "\n",
    "\n",
    "  df.createOrReplaceTempView('temp_in')\n",
    "  \n",
    "  mergeQuery = f\"\"\"\n",
    "  MERGE INTO out_table target\n",
    "  USING temp_in new\n",
    "  ON new.id = target.id\n",
    "  AND new.time = target.time\n",
    "  AND new.date = target.date\n",
    "  AND target.date in ({datas})\n",
    "  WHEN MATCHED\n",
    "  THEN UPDATE SET *\n",
    "  WHEN NOT MATCHED\n",
    "  THEN INSERT *\n",
    "  \"\"\"\n",
    "  df._jdf.sparkSession().sql(mergeQuery)\n",
    "\n",
    "  return None\n",
    "\n",
    "(\n",
    "  dfStream.writeStream\n",
    "  .trigger(once=True)\n",
    "  .foreachBatch(foreachBatchWriteCleansed)\n",
    "  .option(\"checkpointLocation\", CHECKPOINT_LOCATION_PATH)\n",
    "  .start()\n",
    "  .awaitTermination()\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
