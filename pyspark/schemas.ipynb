{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schemas\n",
    "\n",
    "Quando você está lendo uma pasta contendo arquivos texto, em especial json o spark tem que fazer um esforço significativo para abrir uma amostra (ou todos) dos arquivos e identificar o esquema, ou seja, o \"layout\"  do arquivo.\n",
    "Se você ter arquivos seguindo um padrão, dependendo do volume você vai acelerar **e muito** a abertura dos arquivos se especificar o schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemas\n",
    "\n",
    "# Sample function defining a schema\n",
    "def build_schema():\n",
    "    \"\"\"Build and return a schema to use for the sample data.\"\"\"\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"num_pets\", IntegerType(), True),\n",
    "            StructField(\"paid_in_full\", BooleanType(), True),\n",
    "            StructField(\"preferences\", MapType(StringType(), StringType(), True), True),\n",
    "            StructField(\"registered_on\", DateType(), True),\n",
    "            StructField(\"visits\", ArrayType(TimestampType(), True), True),\n",
    "        ]\n",
    "    )\n",
    "    return schema\n",
    "\n",
    "\n",
    "# Create a DataFrame from the RDD, specifying a schema.\n",
    "print(\"RDD: Schema programmatically specified.\")\n",
    "dataDF = spark.createDataFrame(dataRDD, schema=build_schema())\n",
    "dataDF.printSchema()\n",
    "\n",
    "# Create a DataFrame from a JSON source, specifying a schema.\n",
    "print(\"JSON: Schema programmatically specified.\")\n",
    "dataDF = spark.read.json(\"data.json\", schema=build_schema())\n",
    "dataDF.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outra forma de definir schemas\n",
    "\n",
    "Testar melhor essa forma com batch\n",
    "\n",
    "```python\n",
    "\n",
    "schema_tags = \"\"\"\n",
    "tag string,\n",
    "time string,\n",
    "value string,\n",
    "Year string,\n",
    "Month string,\n",
    "Date date\n",
    "\"\"\"\n",
    "\n",
    "dfStream = spark.readStream.format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.format\", \"csv\") \\\n",
    "  .option('sep', ',') \\\n",
    "  .schema(schema_tags) \\\n",
    "  .load(INPUT_PATH)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## samplingRatio\n",
    "\n",
    "Na pior das hipoteses use samplingRatio para diminuir o volume de dados na aferição do schema.\n",
    "\n",
    "```python\n",
    "# Create a DataFrame from a JSON source, inferring the schema from all rows.\n",
    "print(\"JSON: Schema inferred from all rows.\")\n",
    "dataDF = spark.read.option(\"samplingRatio\", 1.0).json(\"data.json\")\n",
    "dataDF.printSchema()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como obter a estrutura de um dataset\n",
    "\n",
    "No exemplo abaixo obtemos um conteúdo JSON da estrutura\n",
    "\n",
    "```python\n",
    "# obtendo a estrutura de um dataset como json\n",
    "import json\n",
    "json_struct = json.dumps(df1.schema.jsonValue(), indent = 2)\n",
    "print(json_struct)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
