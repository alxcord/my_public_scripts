{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alguns exemplos de código pyspark / SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalhando com timezone / timestamp\n",
    "\n",
    "Como trabalhar com campos não UTC (timezone incorporado)\n",
    "\n",
    "```sql\n",
    "-- forma correta, converte a constante para o timestamp de S.P.\n",
    "/* The default format of the Spark Timestamp is yyyy-MM-dd HH:mm:ss.SSSS  */\n",
    "Select time, date_format(from_utc_timestamp(current_timestamp(), 'America/Sao_Paulo'), 'yyyy-MM-dd HH:mm:ss') as current_date_time, count(*)\n",
    "from MyTable\n",
    "where time <= date_format(from_utc_timestamp(current_timestamp(), 'America/Sao_Paulo'), 'yyyy-MM-dd HH:mm:ss') \n",
    "``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatando valores monetários\n",
    "\n",
    "Só trocando o ponto por virgula:\n",
    "\n",
    "```python\n",
    "df.withColumn('Valor', regexp_replace(col('Valor').cast(\"string\"), '\\\\.', '\\\\,'))\n",
    "```\n",
    "\n",
    "Formatação completa \"europeia\" (1.000,00)\n",
    "\n",
    "```python\n",
    "df.withColumn(\"european_format\", regexp_replace(regexp_replace(regexp_replace(\n",
    "            format_number(col(\"column\").cast(\"double\"), 2), '\\\\.', '#'), ',', '\\\\.'), '#', ','))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplicando dados com garantia de pegar o mais recente\n",
    "\n",
    "\n",
    "Jeito \"errado\": Usando o Rank, se houver empate (duas linhas com o mesmo timestamp) cada um terá o rank igual a 1, ou seja, ainda teremos duplicidades.\n",
    "\n",
    "```python\n",
    "my_path = \"/path/to?/data\"\n",
    "window = Window.partitionBy([\"ID\", \"ID_ITEM\"]).orderBy(col(\"TS_UTC_INGENTION\").desc())\n",
    "df = spark.read.csv(my_path) \\\n",
    "      .withColumn(\"rank\", rank().over(window)).filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "```\n",
    "\n",
    "Jeito \"certo\":  usar a função _row_number_ que retornará uma sequencia numérica para cada bloco da clausula window\n",
    "\n",
    "```python\n",
    "Usar \"row_number\" Para ter certeza que vai funcionar quando tiver empate\n",
    "df2 = df.withColumn(\"rank_temp\", row_number().over(window)).filter(col(\"rank_temp\") == 1).drop(\"rank_temp\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando cache de um dataset\n",
    "\n",
    "É util quando se deseja usar o mesmo dataset varias vezes \n",
    "\n",
    "Ver [essa explicação](https://sparkbyexamples.com/spark/spark-dataframe-cache-and-persist-explained/)\n",
    "\n",
    "```python\n",
    " \n",
    "df = ...\n",
    "\n",
    "# Criando estado de dataframe na memoria (cache) \n",
    "df.cache().count()\n",
    "\n",
    "# ... Operações diversas nesse dataframe\n",
    "\n",
    "# REALIZANDO UNPERSIST DO DATAFRAME\n",
    "df.unpersist()\n",
    "...\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminando todo o cache\n",
    "\n",
    "Parece que em alguns casos o spark gera novos caches quando você utiliza um dataset colocado em cache, o que pode gerar problemas de memória em aplicações mais pesadas. A função \"unpersist\" funciona para apagar o cache original mas não resolve dos caches criados automaticamente.\n",
    "\n",
    "Para uma explicação melhor [ver esse post](https://stackoverflow.com/questions/36905717/un-persisting-all-dataframes-in-pyspark#36909862)\n",
    "\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext, HiveContext\n",
    "\n",
    "spark_context = SparkContext(appName='cache_test')\n",
    "hive_context = HiveContext(spark_context)\n",
    "\n",
    "df = (hive_context.read\n",
    "      .format('com.databricks.spark.csv')\n",
    "      .load('simple_data.csv')\n",
    "     )\n",
    "df.cache()\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('C1+C2', df['C1'] + df['C2'])\n",
    "df.cache()\n",
    "df.show()\n",
    "\n",
    "spark_context.stop()\n",
    "```\n",
    "\n",
    "\n",
    "Solução:\n",
    "\n",
    "Spark 2.x: Usar Catalog.clearCache:\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate\n",
    "# ...\n",
    "spark.catalog.clearCache()\n",
    "```\n",
    "\n",
    "Spark 1.x\n",
    "\n",
    "You can use SQLContext.clearCache method which\n",
    "\n",
    "Removes all cached tables from the in-memory cache.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "sqlContext = SQLContext.getOrCreate(SparkContext.getOrCreate())\n",
    "#...\n",
    "sqlContext.clearCache()\n",
    "...\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
