{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Caso de uso: substituição de dados a partir de uma data\r\n",
    "\r\n",
    "Nesse caso devemos atualizar dados a partir de uma fonte que pode ter seus registros eliminados fisicamente, não há qualquer \r\n",
    "sinalização quando um registro é eliminado.\r\n",
    "Entretanto temos uma data de referencia (DT_REFERENCIA) e sabemos que o dados não muda ou é eliminado x dias atras.\r\n",
    "A carga é desacoplada, tratada em duas etapas.\r\n",
    " \r\n",
    "## Requisitos:\r\n",
    "\r\n",
    "### Primeira Etapa (2raw) - não mostrada aqui\r\n",
    "\r\n",
    "- Carga de dados desacoplada, na primeira etapa seleciona os dados dos ultimos n dias a partir de uma data (DT_REFERENCIA) e gravar em um arquivo.\r\n",
    "- Os arquivos devem ser guardados de forma cronológica, particionados pela data de ingestão (DateIngest) seus nomes devem indicar sua cronologia\r\n",
    "- Exemplo: Carga_xyz_yyyyMMDD_hhmmss.snappy.parquet\r\n",
    "\r\n",
    "### Segunda Etapa (raw2cleansed) - Nesse exemplo\r\n",
    "\r\n",
    "- Pegar tudo que foi carregado desde a ultima execução (a partir de DateIngest), aqui mostramos uma forma simplista\r\n",
    "- Garantimos que em cada arquivo entregue pela ingestão é completo, \r\n",
    "  os dados de uma data remessa presente no arquivo mais novo é o que conta, os demais devem ser descartados.\r\n",
    " \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pytz, itertools, unicodedata, json, calendar\r\n",
    "from datetime import datetime, timezone, timedelta, date\r\n",
    "from delta.tables import *\r\n",
    "from pyspark.sql.functions import *\r\n",
    "from pyspark.sql.window import Window\r\n",
    "from pyspark.sql.types import *\r\n",
    "#from functools import reduce"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Obtem o dado da camada de ingestão\r\n",
    "\r\n",
    "last_exec_date = date.today() # Só um exemplo, Usar para isso um sistema de controle de mudança de data\r\n",
    "key_fields = [\"NF\", \"SERIE_NF\"]\r\n",
    "\r\n",
    "my_sourch_path = \"/path/to/source\"\r\n",
    "\r\n",
    "df = spark.read \\\r\n",
    "      .csv(my_sourch_path, header=True) \\\r\n",
    "      .withColumn('SourceFileName', input_file_name())\\\r\n",
    "      .withColumn('ANO_REFERENCIA', substring(\"DT_REFERENCIA\", 1,4)) \\\r\n",
    "      .filter(col(\"DateIngest\") >= last_exec_date)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# para cada data de referencia elimina duplicidades pela pelo nome do arquivo,\r\n",
    "# assim ficamos apenas com o dado do ultimo arquivo.\r\n",
    "# O nome do arquivo pode ser considerado como ordem cronológica.\r\n",
    "# dbfs:/mnt/sadatalakevcbr/raw/Engemix/Command/DadosCPS/YearIngest=2021/MonthIngest=09/DateIngest=2021-09-24/DadosCPS_UTC2021-09-24%20040023__2021-08-10_2021-09-24.csv|2021-09-24 04:00:24 \r\n",
    "\r\n",
    "#df.groupBy(['SOURCEFILENAME', \"TS_UTC_ADF_INGESTION\"]).count().orderBy(['SOURCEFILENAME', \"TS_UTC_ADF_INGESTION\"]).show(1000, False)\r\n",
    "\r\n",
    "w = Window.partitionBy('DT_REFERENCIA')\r\n",
    "df = df.withColumn('maxSourceFileName', max('SourceFileName').over(w)) \\\r\n",
    "       .where(col('maxSourceFileName') == col('SourceFileName')) \\\r\n",
    "       .drop('maxSourceFileName')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove duplicidades pelos campos chave levando em consideração algum outro campo\r\n",
    "window = Window.partitionBy(key_fields).orderBy(desc(col('DATA_ULT_ATUALIZACAO')))\r\n",
    "df = df.withColumn(\"rank_temp\", row_number().over(window)).filter(col(\"rank_temp\") == 1).drop(\"rank_temp\") "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checagem:::: Busca Duplicidades, não deve mostrar nenhum registro\r\n",
    "\r\n",
    "#insert_order_fields = \"TS_UTC_ADF_INGESTION\" # (yyyy-MM-dd HH:mm:ss)\r\n",
    "#window = Window.partitionBy(key_fields).orderBy([desc(order_field) for order_field in insert_order_fields])\r\n",
    "window = Window.partitionBy(key_fields)\r\n",
    "df.withColumn(\"num_temp\", lit(1))\\\r\n",
    "  .withColumn(\"quant_linhas\", sum(col(\"num_temp\")).over(window))\\\r\n",
    "  .drop(\"num_temp\")\\\r\n",
    "  .filter(col(\"quant_linhas\") > 1)\\\r\n",
    "  .show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Criacao da lista de novas datas a serem inseridas\r\n",
    "lista_data_remessa_excluir = [x.DT_REFERENCIA for x in df.select(col(\"DATA_REMESSA\").cast(\"string\")).distinct().orderBy(\"DATA_REMESSA\").collect()]\r\n",
    "lista_data_remessa_excluir = ','.join([f\"'{x}'\" for x in lista_data_remessa_excluir]) # Ex: datas = '\"2020-01-01\", \"2020-01-02\"'\r\n",
    "print(lista_data_remessa_excluir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grava os dados no camada Cleansed\r\n",
    "# antes elimina dados no destino que estejam na lista de datas que vamos inserir. \r\n",
    "# Com isso resolvemos o problema de registros excluídos na origem sendo mantidos no Data Lake\r\n",
    "# Vamos exluir e carregar novamente\r\n",
    "\r\n",
    "# obtem o path do destino\r\n",
    "path_dados_cps_cleansed = ''\r\n",
    "print(path_dados_cps_cleansed)\r\n",
    "\r\n",
    "if DeltaTable.isDeltaTable(spark, path_dados_cps_cleansed):\r\n",
    "  print (\"Atualizando delta table, passo 1/2: Eliminar dados antigos\")\r\n",
    "  delta_table = DeltaTable.forPath(spark, path_dados_cps_cleansed)\r\n",
    "  delta_table.delete(\"DT_REFERENCIA in ({0})\".format(lista_data_remessa_excluir))\r\n",
    "  print (\"Atualizando delta table, passo 2/2: Gravar dados novos\")\r\n",
    "  df.write.format(\"delta\").mode(\"append\").partitionBy('ANO_REFERENCIA').save(path_dados_cps_cleansed)\r\n",
    "else:\r\n",
    "  print (\"Criando delta table\")\r\n",
    "  df.write.format(\"delta\").partitionBy('ANO_REFERENCIA').save(path_dados_cps_cleansed)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}