{"cells":[{"cell_type":"markdown","source":["# Pyspark Guide - Basico\nPor Alex Almeida Cordeiro\n01/2020"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b9166fc-66f4-40de-a6cf-6307280577f8"}}},{"cell_type":"markdown","source":["## O básico\n\n### Executando um Workbook\n\nExecução efêmera, o conteúdo das variáveis será perdido\n```python\ndbutils.notebook.run('/comum/setup_conn_sadatalakevcbr.ipynb',60)\n```\n\nExecução semelhante a um include (uma célula somente com esse código):\n\n```\n%run \"/comum/eng_shared_functions.ipynb\"\n```\n\n### Exibindo o conteúdo de um dataframe:\n```python\ndf.show(100) # lista 100 linhas\ndf.show(100, False) # lista 100 linhas, não trunca campos\n```\n\n### Mostrando estrutura de um dataframe\n```python\ndf.printSchema()\n\n```\n\n\n### \n```python\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fc4b5ed-cb9f-4793-8b46-229f2fa71884"}}},{"cell_type":"markdown","source":["## Recebendo parametros e retornando resultados\nRecebe um parametro, converte para numerico, consiste no caso de parametro invalido, ao final do notebook termina passando Ok como retorno.\n\n```python\ndbutils.widgets.text(\"param_num\", \"\")\ntry:\n  my_param  = int(dbutils.widgets.get(\"param_num\"))\n  flag_no_param = False\nexcept ValueError:\n  my_param = 0\n  flag_no_param = True\nif my_param > 99:\n  raise ValueError(\"Parametros invalidos\")\n\nmuitas_operacoes_depois()\n\ndbutils.notebook.exit(\"Ok\")\n  \n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f74787a-7b76-4a82-a79b-e0a0688ca002"}}},{"cell_type":"markdown","source":["## Funções utilitárias (dbutils.fs)\n\n### Criar diretérios de forma recursiva\n```python\ndbutils.fs.mkdirs('my/path')\n```\n\n\n### Gravando um arquivo simples\nSim, ele vai gerar um arquivo simples, eu podia usar um dataframe ou RDD para isso? Podia mas ia gerar uma pasta com um monte de arquivos dentro.\n```python\nmy_file_content = 'Hello!! Good by!!'\ndbutils.fs.put('my/file/path/and/name.txt', my_file_content, overwrite = True) \n```\n\n### Lendo um arquivo simples (texto)\nOk, está um pouco fora do lugar mas faz sentido estar aqui por causa da gravação\n```python\nrdd = spark.sparkContext.wholeTextFiles('my/file/path/and/name.txt')\nmy_file_content = rdd.collect()[0][1]\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b63b01c-4a55-4a55-81af-ff7c46c2604b"}}},{"cell_type":"markdown","source":["## Lendo e gravando\nTODO: Terminar\n\n### Lendo um dataset para um dataframe\n\n#### json\n\n```python\nspark.read.json('my/folder')\n```\n\n#### csv\n```python\nspark.read.csv('my/folder')\nspark.read.csv('my/folder', header=True) # com cabeçalho\n```\n\n#### parquet\n```python\ndf = spark.read.format(\"parquet\").load('my/folder')\n```\n\n#### delta\n```python\ndf = spark.read.format(\"delta\").load('my/folder')\n```\n\n### Utilizando um esquema predefinido\n```python\nspark.read.schema(my_schema).json(source_dir).createOrReplaceTempView('RawDetailed')\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d6f3bca-3d71-459a-9af3-79c53215e950"}}},{"cell_type":"markdown","source":["## criando um dataframe de teste"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c55b5cd-ef51-4321-bfb1-82392de4c38f"}}},{"cell_type":"code","source":["# https://dwgeek.com/replace-pyspark-dataframe-column-value-methods.html/\nfrom pyspark.sql.functions import *\n\ndata = []\nfor ano in range(2020, 2023):\n  for id in range (100):\n    data.append ((ano, (ano - 2020) * 100 + id, str(id)))\ndf = spark.createDataFrame(data, [\"year\", \"id\", \"d_id\"])\ndf.show(5,False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e18df8a-4161-4fd0-a585-7267bf994f47"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+---+----+\n|year|id |d_id|\n+----+---+----+\n|2020|0  |0   |\n|2020|1  |1   |\n|2020|2  |2   |\n|2020|3  |3   |\n|2020|4  |4   |\n+----+---+----+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+----+\nyear|id |d_id|\n+----+---+----+\n2020|0  |0   |\n2020|1  |1   |\n2020|2  |2   |\n2020|3  |3   |\n2020|4  |4   |\n+----+---+----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Operações com Dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5962ef09-fe5d-4028-a0d0-4fa368d89271"}}},{"cell_type":"markdown","source":["## Manipulando dataframes\n\n### Preenche campos vazios, trata NAs:\nNesse exemplo troca os campos vazios de MyField1 por 0 e de MyField2 por 'NULO'\n```python\nmy_df = my_df.fillna({'MyField1': 0, 'MyField2': 'NULO'})\n```\n\n### Eliminando colunas\nTODO: testar se dá para fazer com uma lista\n\n```python\nmy_df = my_df.drop(col('tag2')).drop(col('time2'))\n```\n\n### Criando um novo campo a partir da Substring de outro\nEsse cria uma coluna ano com os primeiros quatro caracteres de uma data.\n```python\ndf = df.withColumn('Ano', substring('Data', 1,4)) \\\n```\n      \n\n### Join\nEfetuando o join entre dois datasets\n```python\ndf_open_time = df_all.join(df_tag_list, (col('tag') == col('tag2')) & (col('time') >= col('time2')))\n```\n\n### Eliminando duplicidades por uma chave\nElimna duplicidades por tag e time.\n```python\ndf_raw = df_raw.drop_duplicates(subset=['tag', 'time']) \n```\n\n### Exemplos de agregação e seleção\nAqui filtramos uma coluna (time), agrupamos pela TAG agregando time pelo minimo, renomeamos tag para tag2 e a coluna resultante \"min(time)\" para time2\n```python\ndf_tag_list = df_all.filter((col('Next_time') == maximum_timestamp)) \\\n              .filter((col('time') < '9999-12-31 99:99:99')) \\\n              .groupby('tag').agg({'time': 'min'}) \\\n              .select(col('tag').alias('tag2'), col('min(time)').alias('time2')) \n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aedc8283-aa6b-424a-9c20-e91f561209b2"}}},{"cell_type":"markdown","source":["## Exemplos de manipulação avançada\n\n### Clausula Window: diferença entre datas entre dois registros\nAqui vamos calcular a diferença em segundos de um campo data de um registro para o outro a partir de uma chave, ou seja, a partir do campo chave teremos a proxima data\nBaseado em em https://www.arundhaj.com/blog/calculate-difference-with-previous-row-in-pyspark.html\n```python\nfrom pyspark.sql.window import Window\nw_lag = Window.partitionBy(\"Chave\").orderBy(desc('Time'))\nmy_df = my_df.withColumn(\"Next_time\", lag(df_raw['Time']).over(w_lag))\nmy_df = my_df.withColumn( \"Time_gap\", df_raw['Next_time'].cast(\"long\") - df_raw['Time'].cast(\"long\"))\n```\n\n\n### Eliminando chave duplicada pelo menor timestamp\nAqui temos um exemplo onde filtramos um range de datas, criamos uma coluna de ranking por um timestamp agrupado por duas chaves (id_doc, id_item), depois filtramos onde ranking = 1 (somente o timestamp mais atual) e por fim eliminamos o ranking\n```python\nfrom pyspark.sql.window import Window\n\nprocessing_start_date = '2021-01-01'\nprocessing_end_date = '2021-01-05'\ntimestamp_field = 'ts_insert_raw'\nkey_fields = ['id_doc', 'id_item']\nwindow = Window.partitionBy(key_fields).orderBy(col(timestamp_field).desc())\ndf = df.filter(col(raw_layer_date_filter_field)>= processing_start_date) \\\n      .filter(col(raw_layer_date_filter_field)<= processing_end_date) \\\n      .withColumn(\"rank_temp\", rank().over(window)).filter(col(\"rank_temp\") == 1).drop(\"rank_temp\") \n```\n\n\n### Monstrando contagem por mês/ano a partir de uma data\nAinda ordena decrescente\n```python\ndf_DadosCPS.withColumn('ANO_MES_REMESSA', substring('DATA_REMESSA', 1,7)) \\\n  .groupBy('ANO_MES_REMESSA') \\\n  .agg({'RESULTADO':'avg', \"*\":'count'}) \\\n  .select('ANO_MES_REMESSA', col('avg(RESULTADO)').alias('MEDIA_RESULTADO'), col('count(1)').alias('QUANTIDADE')) \\\n  .sort(col('ANO_MES_REMESSA').desc()) \\\n  .show()\n```\n\n```\n+---------------+------------------+----------+\n|ANO_MES_REMESSA|   MEDIA_RESULTADO|QUANTIDADE|\n+---------------+------------------+----------+\n|        2020-12|29.810059438896857|      4206|\n|        2020-11| 33.53508969653114|     19343|\n|        2020-10|34.256294685788326|     23917|\n|        2020-09| 36.06784174470488|     15911|\n|        2020-08|              13.3|         1|\n|        2020-07|              34.3|         4|\n|        2020-05|             43.56|         1|\n|        2020-04|              28.4|         1|\n+---------------+------------------+----------+\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c6b4d3b-93ec-4b37-b59e-7947aca95b33"}}},{"cell_type":"markdown","source":["## Exibição de dados\n\nMontrando um dataframe formatado (data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e693d04-7f86-4fe5-ba84-e53a21ed0bbd"}}},{"cell_type":"markdown","source":["## Mais material\n\n- http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.Column.alias"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b89facc-bdfd-406f-b9ce-4d3e404b0c6a"}}},{"cell_type":"markdown","source":["# SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"265ed1c7-7b0f-4453-b84b-957333898d59"}}},{"cell_type":"markdown","source":["## Criando tabelas e views\n\n### Criando uma view a partir de um dataframe:\n\n```python\nmy_df.createOrReplaceTempView('MyTable')\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a8927cc-c2cc-4246-bd77-1fc2319f18cd"}}},{"cell_type":"markdown","source":["## Rodando código SQL\nDepois que você cria tabelas e views pode rodar SQL normalmente usando %sql na celula, no pyspark pode rodar assim:\n\n```python\nmy_df = spark.sql('SELECT * FROM MINHA_TABELA')\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b43d182-c93a-4a2a-b16d-8d7436b9e3bb"}}},{"cell_type":"markdown","source":["# Delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"029bf6be-dc99-48b8-8a4a-5e5811a10723"}}},{"cell_type":"markdown","source":["## Operações com delta\n\n### Verificando se uma pasta é delta\nretorna True ou False\n``` python\nDeltaTable.isDeltaTable(spark, 'my/path')\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"798b3560-b9e4-4d18-9c6b-2f90a598910c"}}},{"cell_type":"markdown","source":["## Delta: Finalizando o tratamento dos dados\n\n### Otiminzando o tamanho dos arquivos\nElimina arquivos pequenos concatenando-os e gerando arquimos maiores\nhttps://docs.databricks.com/delta/optimizations/file-mgmt.html#delta-optimize\n```sql \n%sql\nOPTIMIZE CleansedPiOsiDetails\n```\nPara rodar do pyspark (creio que não há uma função especifica):\n```python\nfolder = '/path/to/data'\nspark.sql(\"OPTIMIZE delta.`{0}`\".format(folder))\n```\n\n### Elimina arquivos não usados \nElimina arquivos não ativos, predemos o timetravel mas reduzimos consumo espaço, retem 7 dias. \n```sql \n%sql\nVACUUM CleansedPiOsiDetails\n```\n```python\ndeltaTable.vacuum()     # vacuum files not required by versions more than 7 days old\ndeltaTable.vacuum(100)  # vacuum files not required by versions more than 100 hours old\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a20d036-6507-49c3-9fe9-f7a4a32857c9"}}},{"cell_type":"markdown","source":["## Time travel\n\nVamos supor que temos uma tabela delta\n```python\ndelta_df = DeltaTable.forPath(spark, \"/mnt/data/delta\")\n```\n\n### Mostrando a lista de versões\n```python\ndelta_df.history().show(10, True)\n```\n\n\n```\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+\n|version|          timestamp|          userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+\n|      2|2021-01-20 19:41:16|3998117388507181|alex.cordeiro.ac1...|    MERGE|[predicate -> (((...|null|[1481698408020379]|0114-184800-haste129|          1|WriteSerializable|        false|[numTargetRowsCop...|        null|\n|      1|2021-01-20 19:03:00|3998117388507181|alex.cordeiro.ac1...|    MERGE|[predicate -> (((...|null|[1481698408020379]|0114-184800-haste129|          0|WriteSerializable|        false|[numTargetRowsCop...|        null|\n|      0|2021-01-20 19:00:47|3998117388507181|alex.cordeiro.ac1...|    WRITE|[mode -> ErrorIfE...|null|[1481698408020379]|0114-184800-haste129|       null|WriteSerializable|         true|[numFiles -> 1, n...|        null|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+\n```\n\ntrazendo uma versão específica:\n```python\nversion_1 = spark.read.format(\"delta\").option(\"versionAsOf\",1).load(\"/mnt/data/delta\")\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ef6c24f-527f-4f9c-be37-d6451c06c8b2"}}},{"cell_type":"markdown","source":["## Delta: Mais material\n- https://docs.delta.io/latest/delta-utility.html#table-utility-commands"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"818b1c5f-78a8-4532-a2bc-de6996b0f47e"}}},{"cell_type":"markdown","source":["# Metadados"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b76bde9-9459-48cf-89bb-ca044002e6da"}}},{"cell_type":"markdown","source":["## Metadados - StructField\nDocumentação util:\n- https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/\n\n### Criando uma estrutura\n\n```python\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nschema = StructType([ \n    StructField(\"firstname\",StringType(),True), \n    StructField(\"middlename\",StringType(),True), \n    StructField(\"lastname\",StringType(),True), \n    StructField(\"id\", StringType(), True), \n    StructField(\"gender\", StringType(), True), \n    StructField(\"salary\", IntegerType(), True) \n  ])\n```\n\n\n### Verificando se um campo existe em um Dataframe\nSe você deseja realizar algumas verificações nos metadados do DataFrame, por exemplo, se uma coluna ou campo existe em um DataFrame ou tipo de dados da coluna; podemos fazer isso facilmente usando várias funções em SQL StructType e StructField.\n```python\nprint(df.schema.fieldNames.contains(\"firstname\"))\nprint(df.schema.contains(StructField(\"firstname\",StringType,true)))\n```\n\n### Alterando o metadado de um Dataframe\nTambém pode ser usado para renomear\n```python\ndef set_metadata(my_metadata):\n  my_metadata[\"Teste\"] = \"Ok\"\n  return my_metadata\n\nnew_df = df\nnew_df = new_df.select([col(c).alias(c, metadata = set_metadata(new_df.schema[c].metadata)  ) for c in new_df.columns])\n\nfor sc_col in new_df.schema:\n  print (sc_col.jsonValue())\n```\nTambém pode ser usado para renomear as colunas com:\n```python\nnew_df = new_df.select([col(c).alias(c + \"_new\", metadata = set_metadata(new_df.schema[c])  ) for c in \n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4b2919b-7a55-4481-8382-5b371fc7c2e7"}}},{"cell_type":"code","source":["# https://dwgeek.com/replace-pyspark-dataframe-column-value-methods.html/\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\n\nschema = StructType([ \n    StructField(\"firstname\",StringType(),True, metadata = {\"desc\":\"Nome\"}), \n    StructField(\"middlename\",StringType(),True), \n    StructField(\"lastname\",StringType(),True), \n    StructField(\"id\", StringType(), True), \n    StructField(\"gender\", StringType(), True), \n    StructField(\"salary\", IntegerType(), True) \n  ])\n\ndata = [(\"João\", \"Milho\", \"de Freitas\", 1, \"M\", 2000),\n        (\"Maria\", \"de Lourdes\", \"de Freitas\", 1, \"M\", 3000),\n        (\"Michael\",\"\", \"Rose\",\"40288\",\"M\",4000),\n        (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",3000),\n       ]\n\ndf = spark.createDataFrame(data, schema )\ndf.show()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0918cef2-03ed-4187-9808-41623850fea6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------+----------+----------+-----+------+------+\n|firstname|middlename|  lastname|   id|gender|salary|\n+---------+----------+----------+-----+------+------+\n|     João|     Milho|de Freitas|    1|     M|  2000|\n|    Maria|de Lourdes|de Freitas|    1|     M|  3000|\n|  Michael|          |      Rose|40288|     M|  4000|\n|   Robert|          |  Williams|42114|     M|  4000|\n|    Maria|      Anne|     Jones|39192|     F|  4000|\n|      Jen|      Mary|     Brown|     |     F|  3000|\n+---------+----------+----------+-----+------+------+\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+----------+----------+-----+------+------+\nfirstname|middlename|  lastname|   id|gender|salary|\n+---------+----------+----------+-----+------+------+\n     João|     Milho|de Freitas|    1|     M|  2000|\n    Maria|de Lourdes|de Freitas|    1|     M|  3000|\n  Michael|          |      Rose|40288|     M|  4000|\n   Robert|          |  Williams|42114|     M|  4000|\n    Maria|      Anne|     Jones|39192|     F|  4000|\n      Jen|      Mary|     Brown|     |     F|  3000|\n+---------+----------+----------+-----+------+------+\n\nroot\n-- firstname: string (nullable = true)\n-- middlename: string (nullable = true)\n-- lastname: string (nullable = true)\n-- id: string (nullable = true)\n-- gender: string (nullable = true)\n-- salary: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def set_metadata(my_metadata):\n  my_metadata[\"Teste\"] = \"Ok\"\n  return my_metadata\n\nnew_df = df\nnew_df = new_df.select([col(c).alias(c, metadata = set_metadata(new_df.schema[c].metadata)  ) for c in new_df.columns])\n\nfor sc_col in new_df.schema:\n  print (sc_col.jsonValue())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Alterando e mostrando o metadado de um dataframe","showTitle":true,"inputWidgets":{},"nuid":"fe0c9bd7-6b75-489b-9929-fb4af91f6fc4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">{&#39;name&#39;: &#39;firstname&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;, &#39;desc&#39;: &#39;Nome&#39;}}\n{&#39;name&#39;: &#39;middlename&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;}}\n{&#39;name&#39;: &#39;lastname&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;}}\n{&#39;name&#39;: &#39;id&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;}}\n{&#39;name&#39;: &#39;gender&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;}}\n{&#39;name&#39;: &#39;salary&#39;, &#39;type&#39;: &#39;integer&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;}}\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;name&#39;: &#39;firstname&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;, &#39;desc&#39;: &#39;Nome&#39;}}\n{&#39;name&#39;: &#39;middlename&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;}}\n{&#39;name&#39;: &#39;lastname&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;}}\n{&#39;name&#39;: &#39;id&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;}}\n{&#39;name&#39;: &#39;gender&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;}}\n{&#39;name&#39;: &#39;salary&#39;, &#39;type&#39;: &#39;integer&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;Teste&#39;: &#39;Ok&#39;}}\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["for sc_col in new_df.schema:\n  if \"newname\" in sc_col.metadata and sc_col.metadata[\"newname\"] != \"\" and sc_col.metadata[\"newname\"] != sc_col.name:\n    new_df = new_df.withColumnRenamed(sc_col.metadata[\"newname\"], col(sc_col.name))\n  \nfor sc_col in new_df.schema:\n  print (sc_col.jsonValue())                             \nnew_df.show(2)\nnew_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Renomeando uma coluna com base no metadado","showTitle":true,"inputWidgets":{},"nuid":"60db1e97-6e01-48b9-b326-1243ccfe2b57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">{&#39;name&#39;: &#39;year_new&#39;, &#39;type&#39;: &#39;long&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;comment&#39;: &#39;deu certo&#39;, &#39;newname&#39;: &#39;year_new&#39;}}\n{&#39;name&#39;: &#39;id_new&#39;, &#39;type&#39;: &#39;long&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;comment&#39;: &#39;deu certo&#39;, &#39;newname&#39;: &#39;id_new&#39;}}\n{&#39;name&#39;: &#39;d_id_new&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;comment&#39;: &#39;deu certo&#39;, &#39;newname&#39;: &#39;d_id_new&#39;}}\n+--------+------+--------+\n|year_new|id_new|d_id_new|\n+--------+------+--------+\n|    2020|     0|       0|\n|    2020|     1|       1|\n+--------+------+--------+\nonly showing top 2 rows\n\nroot\n |-- year_new: long (nullable = true)\n |-- id_new: long (nullable = true)\n |-- d_id_new: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;name&#39;: &#39;year_new&#39;, &#39;type&#39;: &#39;long&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;comment&#39;: &#39;deu certo&#39;, &#39;newname&#39;: &#39;year_new&#39;}}\n{&#39;name&#39;: &#39;id_new&#39;, &#39;type&#39;: &#39;long&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;comment&#39;: &#39;deu certo&#39;, &#39;newname&#39;: &#39;id_new&#39;}}\n{&#39;name&#39;: &#39;d_id_new&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {&#39;comment&#39;: &#39;deu certo&#39;, &#39;newname&#39;: &#39;d_id_new&#39;}}\n+--------+------+--------+\nyear_new|id_new|d_id_new|\n+--------+------+--------+\n    2020|     0|       0|\n    2020|     1|       1|\n+--------+------+--------+\nonly showing top 2 rows\n\nroot\n-- year_new: long (nullable = true)\n-- id_new: long (nullable = true)\n-- d_id_new: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Spark API - Metadados de tabelas\nTODO: Checar & testar\n\nVer https://www.learningjournal.guru/courses/spark/spark-foundation-training/spark-data-types-and-metadata/\n\nEsse código é scala???\n```python\nspark.catalog.listDatabases\n// same as SHOW DATABASES\n//This API gives you a dataset for the list of all databases. You can display the list using the show method.\nspark.catalog.listDatabases.show\n//You can collect it back to the master node as a Scala Array.\nval dbs = spark.catalog.listDatabases.collect\n//Then you can loop through the array and apply a function on each element. Let's apply the println.\ndbs.foreach(println)\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf011ae3-c188-4d29-9fb1-dfe36faed58f"}}},{"cell_type":"markdown","source":["## Mais operações com metadados\n\n### Como manter o metadado durante um ajuste\nAo ajustar uma coluna com o withColumn, por exemplo, todos os seus Metadados são perdidos.\n\n```python\ndf = df.withColumn('NF', rtrim('NF').alias(\"\", metadata = df.schema['NF'].metadata)) \\\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09cc99c2-27b5-42eb-b9ca-fbb76dc3fd3b"}}},{"cell_type":"markdown","source":["# Outros"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5720b64d-335c-4795-af2e-90afdcc48159"}}},{"cell_type":"markdown","source":["## Outras operações interessantes\n\n### Criando uma função aplicavel a um dataframe\nVer \nhttps://stackoverflow.com/questions/46667810/how-to-update-pyspark-dataframe-metadata-on-spark-2-1\n\n```python\ndef withMeta(self, alias, meta):\n    sc = SparkContext._active_spark_context\n    jmeta = sc._gateway.jvm.org.apache.spark.sql.types.Metadata\n    return Column(getattr(self._jc, \"as\")(alias, jmeta.fromJson(json.dumps(meta))))\n\nColumn.withMeta = withMeta\n\n# new metadata:\nmeta = {\"ml_attr\": {\"name\": \"label_with_meta\",\n                    \"type\": \"nominal\",\n                    \"vals\": [str(x) for x in range(6)]}}\n\ndf_with_meta = df.withColumn(\"label_with_meta\", col(\"label\").withMeta(\"\", meta))\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c190721-2f3b-4bb7-948d-a3c63dd9ca09"}}},{"cell_type":"markdown","source":["## Erros e soluções\n\n### TypeError: 'StructField' object is not callable usando a função col\nExemplo: df.filter(col('CAMPO') >= 1234).show()\nOlhe no seu cósigo se você tem alguma função que usa uma variável chamada col..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b27cc1bc-12b1-43db-9eae-cf263965914a"}}},{"cell_type":"markdown","source":["## Mais material\n\n- spark-gotchas -- https://github.com/awesome-spark/spark-gotchas/blob/master/06_data_preparation.md#python\n- https://data.solita.fi/pyspark-execution-logic-code-optimization/\n- https://medium.com/analytics-vidhya/getting-hands-dirty-in-spark-delta-lake-1963921e4de6"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54c6a914-b339-4769-9f92-72a10177339e"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark_guide.ipynb","dashboards":[],"language":"python","widgets":{},"notebookOrigID":3643891458986797}},"nbformat":4,"nbformat_minor":0}
